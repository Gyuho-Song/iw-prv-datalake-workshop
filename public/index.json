[
{
	"uri": "/prvdlworkshop/",
	"title": "홈",
	"tags": [],
	"description": "",
	"content": "부장님도 좋아하는 AWS의 안전한 Private Data Lake  AWS Cloud Week 2020 - Private data lake HOL workshop 안녕하세요 [부장님도 좋아하는 AWS의 안전한 Private Data Lake] 세션을 발표한 신중훈 \u0026amp; 송규호입니다. 세션 데모를 Self Phase Lab으로 제공하는 페이지입니다. 데이터 레이크를 Private link를 이용해서 구성하고 Lake Formation을 이용한 권한 관리를 실습해보실 수 있습니다. 실습을 진행할 때 사용하는 서비스에 대한 비용이 청구될 수 있는 점 유의 부탁드립니다. AWS SA와 함께 무료로 Workshop을 진행해보고 싶으신 고객분들께서는 담당 AM들에게 문의 해주시기 바랍니다.\nArchitecture  "
},
{
	"uri": "/prvdlworkshop/lab0/",
	"title": "실습1. HOL 환경 구성",
	"tags": [],
	"description": "",
	"content": "실습을 진행할 두개의 VPC(Source와 Target)를 구성합니다. Source VPC는 고객사의 On-premise를 시뮬레이션한 VPC이고 Target VPC는 Private DataLake로 사용할 VPC입니다. 각각의 VPC에는 Public subnet과 Private subnet이 있으며 외부 통신을 위한 NAT Gateway가 구성됩니다. 이 단계는 AWS CloudFormation을 이용하여 진행합니다. 이후에는 실습 데이터를 저장할 Aurora Database를 생성하고 서비스간 통신을 위한 Private endpoint를 구성합니다. Table of Contents  실습 데이터 셋 정보 VPC 생성 Aurora 데이터 베이스 생성 Aurora 셋업 및 데이터셋 로드 Private Endpoint 설정  실습 데이터셋 정보 Kaggle - Fannie Mae \u0026amp; Freddie Mac Database 2008-2018 Original dataset link\n본 실습에서는 이 중 Single Family Properties Census Tract file 2013 ~ 2018 데이터셋만을 사용했습니다. VPC 생성 실습을 진행할 VPC를 생성하고 필요한 환경설정을 진행합니다. 미리 준비된 CloudFormation을 이용하여 배포합니다.\n  아래의 링크를 클릭하여 CloudFormation Template을 수행합니다.\n  AWS Console에서 CloudFormation 수행 상태를 확인하고 Deploy가 완료되면 다음 단계로 진행합니다.\n  AWS Console에서 VPC 구성을 확인합니다.\n  Aurora 데이터 베이스 생성 원천 데이터를 저장해놓을 Aurora 데이터베이스를 생성합니다.\n  Amazon RDS 페이지로 이동합니다.   다음 정보를 참조해서 데이터 베이스를 생성합니다.\n  데이터베이스 생성 방식 선택 : 표준 생성\r엔진 옵션 : Amazon Aurora\r에디션 : MySQL과 호환되는 Amazon Aurora\r용량 유형: 프로비저닝됨\r버전 : Aurora (MySQL 5.7) 2.07.0 이상\rDB 클러스터 식별자 : prv-dl-db-cluster\r마스터 사용자 이름 : admin\r마스터 암호 : awsiw2020\r연결 VPC 정보 : 이전 단계에서 생성한 Source VPC\r데이터 베이스 인증 옵션 : 암호 인증\r추가 구성 \u0026gt; 초기 데이터베이스 이름: srcdb\r연결 VPC 정보에 특히 유의합니다. (앞 단계에서 CloudFormation으로 생성한 Source VPC를 선택해야함)  추가 연결 구성 항목을 펼쳐서 VPC 보안 그룹에 CloudFormation에서 미리 생성한 \u0026lsquo;iw-aurora-\u0026lsquo;로 시작하는 항목을 선택합니다.  Amazon Aurora 클러스터가 생성되면 엔드포인트를 따로 복사합니다.  Aurora 셋업 및 데이터셋 로드 실습에 사용할 데이터셋을 다운받아서 Aurora 데이터베이스로 적재합니다.\n Aurora 데이터베이스에 접속하여 작업을 할 EC2 Instance에 ssh로 접속합니다. EC2 정보는 EC2 console에서 확인할 수 있습니다. Source VPC에 있는 EC2에 접속합니다.   ssh -i \u0026lt;ssh private key\u0026gt; ec2-user@\u0026lt;instance ip or dns name\u0026gt;\rAWS cli 설정을 합니다.  aws configure\r실습에서 사용할 S3 버킷을 생성합니다.  aws s3 mb s3://prv-dl-\u0026lt;USERID\u0026gt; --region ap-northeast-2\r실습에서 사용할 데이터를 다운로드 하고 압축을 풉니다.  wget \u0026quot;https://private-data-lake-iw2020.s3.ap-northeast-2.amazonaws.com/sfct_dataset/SFCT.tar.gz\u0026quot;\rtar xvfz SFCT.tar.gz\rEC2에 mysql을 설치하고 Aurora에 접속합니다. 아까 복사해둔 Aurora database의 endpoint를 이용합니다.  sudo yum install mysql\rmysql -h \u0026lt;Aurora database endpoint\u0026gt; -P 3306 -u admin -p\r암호를 묻는 창에는 아까 설정한 \u0026lsquo;awsiw2020\u0026rsquo;를 입력합니다.\nDatabase schema와 user를 생성합니다.  CREATE DATABASE estate;\rCREATE USER 'user1'@'%' IDENTIFIED BY 'password1';\rGRANT SELECT, INSERT, UPDATE, DELETE, CREATE, DROP, RELOAD, PROCESS, REFERENCES, INDEX, ALTER, SHOW DATABASES, CREATE TEMPORARY TABLES, LOCK TABLES, EXECUTE, REPLICATION SLAVE, REPLICATION CLIENT, CREATE VIEW, SHOW VIEW, CREATE ROUTINE, ALTER ROUTINE, CREATE USER, EVENT, TRIGGER ON *.* TO 'user1'@'%' WITH GRANT OPTION;\rTable을 생성하고 데이터를 Table로 load합니다.  USE estate;\rCREATE TABLE SFCT (\rent_flag int,\rrecord_num int,\rus_pos_code int,\rmsa_code int,\rcountry_code int,\rcensus_tract int,\rminority_pct float,\rmedian_income int,\rlocal_med_income int,\rincome_ratio float,\rborrower_income int,\rarea_med_income int,\rborrower_income_ratio float,\rupb int,\rloan_purpose int,\rfederal_guarntee int,\rnum_borrower int,\rfirst_buyer int,\rborrower_race1 int,\rborrower_race2 int,\rborrower_race3 int,\rborrower_race4 int,\rborrower_race5 int,\rborrower_ethnicity int,\rco_borrower_race1 int,\rco_borrower_race2 int,\rco_borrower_race3 int,\rco_borrower_race4 int,\rco_borrower_race5 int,\rco_borrower_ethnicity int,\rborrower_gender int,\rco_borrower_gender int,\rage_borrower int,\rage_co_borrower int,\roccupancy_code int,\rrate_spread float,\rhoepa_status int,\rproperty_type int,\rlien_status int\r);\rLOAD DATA LOCAL INFILE '/home/ec2-user/SFCT/2013/fhlmc_sf2013c_loans.txt' INTO TABLE SFCT\rFIELDS TERMINATED BY ' '\rOPTIONALLY ENCLOSED BY '\u0026quot;';\rLOAD DATA LOCAL INFILE '/home/ec2-user/SFCT/2013/fnma_sf2013c_loans.txt' INTO TABLE SFCT\rFIELDS TERMINATED BY ' '\rOPTIONALLY ENCLOSED BY '\u0026quot;';\rLOAD DATA LOCAL INFILE '/home/ec2-user/SFCT/2014/fhlmc_sf2014c_loans.txt' INTO TABLE SFCT\rFIELDS TERMINATED BY ' '\rOPTIONALLY ENCLOSED BY '\u0026quot;';\rLOAD DATA LOCAL INFILE '/home/ec2-user/SFCT/2014/fnma_sf2014c_loans.txt' INTO TABLE SFCT\rFIELDS TERMINATED BY ' '\rOPTIONALLY ENCLOSED BY '\u0026quot;';\rLOAD DATA LOCAL INFILE '/home/ec2-user/SFCT/2015/fhlmc_sf2015c_loans.txt' INTO TABLE SFCT\rFIELDS TERMINATED BY ' '\rOPTIONALLY ENCLOSED BY '\u0026quot;';\rLOAD DATA LOCAL INFILE '/home/ec2-user/SFCT/2015/fnma_sf2015c_loans.txt' INTO TABLE SFCT\rFIELDS TERMINATED BY ' '\rOPTIONALLY ENCLOSED BY '\u0026quot;';\rLOAD DATA LOCAL INFILE '/home/ec2-user/SFCT/2016/fhlmc_sf2016c_loans.txt' INTO TABLE SFCT\rFIELDS TERMINATED BY ' '\rOPTIONALLY ENCLOSED BY '\u0026quot;';\rLOAD DATA LOCAL INFILE '/home/ec2-user/SFCT/2016/fnma_sf2016c_loans.txt' INTO TABLE SFCT\rFIELDS TERMINATED BY ' '\rOPTIONALLY ENCLOSED BY '\u0026quot;';\rLOAD DATA LOCAL INFILE '/home/ec2-user/SFCT/2017/fhlmc_sf2017c_loans.txt' INTO TABLE SFCT\rFIELDS TERMINATED BY ' '\rOPTIONALLY ENCLOSED BY '\u0026quot;';\rLOAD DATA LOCAL INFILE '/home/ec2-user/SFCT/2017/fnma_sf2017c_loans.txt' INTO TABLE SFCT\rFIELDS TERMINATED BY ' '\rOPTIONALLY ENCLOSED BY '\u0026quot;';\rLOAD DATA LOCAL INFILE '/home/ec2-user/SFCT/2018/fhlmc_sf2018c_loans.txt' INTO TABLE SFCT\rFIELDS TERMINATED BY ' '\rOPTIONALLY ENCLOSED BY '\u0026quot;';\rLOAD DATA LOCAL INFILE '/home/ec2-user/SFCT/2018/fnma_sf2018c_loans.txt' INTO TABLE SFCT\rFIELDS TERMINATED BY ' '\rOPTIONALLY ENCLOSED BY '\u0026quot;';\rTable에 저장된 데이터를 살펴봅니다. Dictionary 데이터를 매핑하기 전이기 때문에 모든 값이 숫자형으로 되어 있는 것을 확인할 수 있습니다.  select * from SFCT limit 10;\rPrivate Endpoint 설정 AWS의 내부망을 사용하여 데이터를 전송하는데 꼭 필요한 Private Endpoint들을 생성합니다. S3와 Glue의 엔드포인트가 필요합니다.\n  AWS Console에서 VPC Service로 이동합니다.   엔드포인트 항목을 선택하여 S3 endpoint를 만듭니다. 주의! VPC 선택창에서 실습에서 사용할 Target VPC를 꼭 선택하셔야 합니다.   Glue endpoint도 만듭니다. 마찬가지로 VPC 선택창에서 실습에서 사용할 Target VPC를 꼭 선택하셔야 합니다.   실습2에서 계속  "
},
{
	"uri": "/prvdlworkshop/lab1/",
	"title": "실습2. Glue를 활용한 데이터 ETL 작업 수행",
	"tags": [],
	"description": "",
	"content": "Glue를 사용하기 위한 환경 설정을 하고 이를 이용해서 데이터를 Loading합니다. 이후에는 Glue Dev Endpoint 노트북을 이용하여 Dictionary 데이터를 매핑시키는 데이터 ETL 작업을 진행합니다. Table of Contents  Glue 실행을 위한 IAM role 생성 Glue Security Group 생성 Glue를 이용한 데이터 Crawling Glue를 이용한 데이터 loading 작업 Glue Dev endpoint Notebook을 활용한 Dictionary 데이터 매핑  Glue 실행을 위한 IAM role 생성 Glue를 실행하기 위한 IAM role을 생성합니다.\n  AWS 서비스 콘솔에서 IAM service로 이동합니다.   역할만들기 유형에서 AWS 서비스가 선택되어 있는지 확인합니다.   화면 하단에서 Glue를 선택합니다.   권한 정책 연결에서 다음의 정책들을 선택합니다.\n  AmazonRDSFullAccess\rAmazonS3FullAccess\rAWSGlueServiceRole\r역할 이름에 prv-dl-workshop-role을 입력하고 생성합니다.   Glue Security Group 생성 Glue의 데이터 통신을 위한 Security Group 설정을 합니다.\n AWS 콘솔에서 VPC서비스로 이동한 뒤 Security Group에서 Glue가 사용할 Security Group을 생성합니다.  보안 그룹 생성 버튼을 눌러서 Security Group을 만듭니다. 하단의 정보를 참고합니다.  보안 그룹 이름: prv-dl-glue-sg\r설명: Security Group for Glue ETL in prv dl workshop\rVPC: Target VPC\r보안 그룹이 생성되면 인바운드 규칙 편집 버튼을 눌러서 self referencing 룰을 추가해줍니다. 방금 생성한 Glue Security Group자신을 소스로하는 모든 트래픽을 허용하는 룰입니다.   Glue를 이용한 데이터 Crawling Glue를 이용해서 데이터를 처리하려면 데이터 소스에 대한 정보를 등록해주는 작업을 선행해야합니다.\n AWS Glue 서비스 콘솔로 이동한 뒤 연결 페이지로 이동합니다. 연결 추가를 눌러서 Aurora 데이터베이스의 Connection 정보를 등록합니다.  연결 구성에 다음의 정보를 참조합니다.  연결 이름: prv-dl-aurora\r연결 유형: Amazon RDS\r데이터 베이스 엔진: Amazon Aurora\r인스턴스: prv-dl-db-cluster-instance-1\r데이터베이스 이름: srcdb\r사용자 이름: admin\r암호: awsiw2020\r다음으로는 크롤러를 추가해줍니다. 크롤러 페이지에서 크롤러 추가 버튼을 클릭합니다.  다음의 정보를 참조하여 크롤러를 생성합니다.  크롤러 이름: prv-dl-aurora-crawler\rCrawler source type: Data stores\rRepeat crawls of S3 data stores: Crawl all folders\r데이터 스토어 선택: JDBC\r연결: prv-dl-aurora\r포함경로: estate/%\r다른 데이터 스토어 추가: 아니요\rIAM 역할: prv-dl-workshop-role\r빈도: 온디맨드 실행\r데이터 베이스: 데이터베이스 추가 버튼을 누르고 prv-dl-db 입력\r 방금 생성한 크롤러를 선택하고 크롤러 실행 버튼을 클릭   등록된 Aurora 데이터 베이스 테이블을 확인   Glue를 이용한 데이터 loading 작업 등록되어 있는 Aurora Table에 있는 데이터를 S3로 가져오는 작업을 수행합니다.\n  AWS Glue 콘솔에서 ETL 카테고리 아래에 있는 작업 페이지로 들어갑니다.   작업 추가 버튼을 눌러서 작업을 등록합니다. 아래의 정보를 참조합니다.\n  이름: prv-dl-glue-job-sfct-load-to-s3\rIAM 역할: prv-dl-workshop-role\rType: Spark\rGlue Version: Spark 2.4, Python 3 with improved job startup times (Glue Version 2.0)\r작업 실행 대상: AWS Glue가 생성하여 제안하는 스크립트\r스크립트 파일 이름: prv-dl-glue-job-sfct-load-job\r데이터 원본: estate_sfct\r변환 유형 선택: 스키마 변경\r데이터 스토어: Amazon S3\r형식: parquet\r연결: 연결 추가 버튼을 클릭해서 새로운 S3 연결 생성\r연결 이름: prv-dl-connection-to-s3\r연결 VPC: Target VPC\r연결 서브넷: Target VPC의 private subnet 중 하나\r연결 보안 그룹: prv-dl-glue-sg\r대상 경로: \u0026lt;이전 Step에서 생성한 S3 bucket의 하위 폴더를 지정\u0026gt;: s3://prv-dl-\u0026lt;USERID\u0026gt;/rawdata\rOutput Schema Definition: 작업 저장 및 스크립트 편집 버튼 클릭\r스키마 편집 화면에서 우측 상단에 있는 X 버튼을 눌러서 창을 닫음\r 등록된 작업을 선택하고 작업 실행 버튼을 클릭합니다.   작업이 완료되면 대상 S3 버킷에서 데이터를 확인합니다.   S3로 로딩된 데이터를 Crawler를 이용하여 Glue Catalog에 다시 추가합니다. 크롤러 페이지에서 크롤러 추가 버튼을 클릭합니다.   다음의 정보를 참조하여 크롤러를 생성합니다.\n  크롤러 이름: prv-dl-rawdata-crawler\rCrawler source type: Data stores\rRepeat crawls of S3 data stores: Crawl all folders\r데이터 스토어 선택: S3\r연결: prv-dl-connection-to-s3\r포함경로: s3://prv-dl-\u0026lt;USERID\u0026gt;/rawdata\r다른 데이터 스토어 추가: 아니요\rIAM 역할: prv-dl-workshop-role\r빈도: 온디맨드 실행\r데이터 베이스: prv-dl-db\r 방금 생성한 크롤러를 선택하고 크롤러 실행 버튼을 클릭   S3로 로딩된 데이터가 Glue Catalog Table에 등록되어 있는 것을 확인   Glue Dev endpoint Notebook을 활용한 Dictionary 데이터 매핑 Raw 데이터에 SFCT의 Dictionary 정보를 매핑 시켜서 분석 결과를 직관적으로 이해할 수 있게 변환합니다. Dictionary 정보는 Kaggle의 Fannie Mae \u0026amp; Freddie Mac Database 2008-2018 페이지에서 pdf 파일로 제공합니다. 작업을 간편하게 하기 위해서 Glue의 DevEndpoint 노트북을 사용했습니다.\n AWS Glue 콘솔에서 개발 엔드포인트 페이지로 들어간 후 엔드포인트 추가 버튼을 클릭합니다.  아래 정보를 참조해서 개발 엔드포인트를 생성합니다.  개발 엔드포인트 이름: prv-dl-glue-dev-ep\rIAM 역할: prv-dl-workshop-role\r네트워킹: VPC, 서브넷 및 보안 그룹 선택\rVPC: Target VPC\r서브넷: Target VPC의 private subnet 중 하나\r보안 그룹: prv-dl-glue-sg\r노트북 페이지에서 SageMaker 노트북 탭을 선택하고 노트북 생성 버튼을 클릭합니다.  아래 정보를 참조해서 노트북을 생성합니다.  노트북 이름: aws-glue-prv-dl-notebook\r개발 엔드포인트에 연결: prv-dl-glue-dev-ep\rIAM 역할: IAM 역할 생성 선택 후 이름은 끝에 prv-dl-role을 추가합니다.\rVPC: Target VPC\r서브넷: Target VPC의 private subnet 중 하나\r보안 그룹: prv-dl-glue-sg\r Glue notebook Role에 S3에 접근 할 수 있는 권한을 부여하기 위해서 AWS 서비스 콘솔에서 IAM service로 이동합니다.   검색창에 prv-dl으로 검색해서 방금 생성한 AWSGlueServiceSageMakerNotebookRole-prv-dl-role을 선택합니다.   정책 연결 버튼을 눌러서 policy를 추가합니다.   검색창에 s3full을 입력해서 AmazonS3FullAccess를 선택하고 옆에 있는 체크박스를 클릭한 뒤 정책 연결 버튼을 클릭합니다.   실습을 진행하고 있는 Labtop PC 웹브라우져의 주소창에 다음의 url을 입력해서 노트북을 다운받습니다.\n  https://private-data-lake-iw2020.s3.ap-northeast-2.amazonaws.com/sfct_dataset/prv-dl-etl-transform.ipynb\r 다시 Glue 서비스 콘솔로 돌아온 뒤 생성한 노트북을 엽니다. 노트북 페이지에서 노트북 열기 버튼을 클릭합니다.   다운받았던 prv-dl-etl-transform.ipynb 노트북을 업로드 버튼을 눌러서 올립니다.   prv-dl-etl-transform.ipynb을 클릭해서 내용을 확인합니다. 셀의 내용을 확인하면서 차례 차례 실행합니다. 한번에 전부 수행하시려면 메뉴에 있는 Cell 버튼을 눌러서 메뉴를 펼치고 Run All을 클릭합니다.   마지막 부분의 Cell에 있는 S3 bucket명을 현재 실습을 진행하고 있는 버킷으로 바꿔줍니다.   수행이 완료되면 실습을 진행하고 있는 S3 버킷의 dictionaried_data 폴더에 데이터가 정상적으로 처리되었는지 확인합니다.   S3로 로딩된 데이터를 Crawler를 이용하여 Glue Catalog에 다시 추가합니다. 크롤러 페이지에서 크롤러 추가 버튼을 클릭합니다.   다음의 정보를 참조하여 크롤러를 생성합니다.\n  크롤러 이름: prv-dl-rawdata-crawler\rCrawler source type: Data stores\rRepeat crawls of S3 data stores: Crawl all folders\r데이터 스토어 선택: S3\r연결: prv-dl-connection-to-s3\r포함경로: s3://prv-dl-\u0026lt;USERID\u0026gt;/dictionaried_data\r다른 데이터 스토어 추가: 아니요\rIAM 역할: prv-dl-workshop-role\r빈도: 온디맨드 실행\r데이터 베이스: prv-dl-db\r 방금 생성한 크롤러를 선택하고 크롤러 실행 버튼을 클릭합니다.   S3로 로딩된 데이터가 Glue Catalog Table에 등록되어 있는 것을 확인합니다.   실습3에서 계속  "
},
{
	"uri": "/prvdlworkshop/lab2/",
	"title": "실습3. LakeFormation을 활용한 권한 설정",
	"tags": [],
	"description": "",
	"content": "이전 단계에서 처리한 데이터 중 민감한 데이터 컬럼에 대한 권한을 LakeFormation을 이용하여 제어합니다. 본 실습에서는 인종 정보가 포함된 borrower_race컬럼들과 co_borrower_race 컬럼들을 민감한 데이터라고 가정했습니다. Table of Contents  RedShift Spectrum이 사용할 Role 생성 LakeFormation으로 Redshift Spectrum에서 접근 가능한 컬럼의 권한 설정 Redshift 클러스터 생성 RedShift 접속 환경 설정 RedShift Spectrum을 이용한 데이터 loading  RedShift Spectrum이 사용할 Role 생성 IAM 서비스에서 RedShift Spectrum이 사용할 Role을 생성합니다.\n  IAM 서비스 콘솔로 이동합니다.\n  역할만들기 유형에서 AWS 서비스가 선택되어 있는지 확인합니다.   화면 하단에서 Redshift를 선택합니다. 사용 사례는 Redshift - Customizable 선택합니다.   권한 정책 연결에서 다음의 정책들을 선택합니다.\n  AmazonRedshiftFullAccess\rAmazonS3FullAccess\r 역할 이름에 prv-dl-redshift-role을 입력하고 역할을 생성합니다.\n  생성한 role의 역할 ARN값을 복사해둡니다.\n  LakeFormation으로 Redshift Spectrum에서 접근 가능한 컬럼의 권한 설정 이전 단계에서 생성한 rawdata table에 있는 컬럼중에 Redshift Spectrum이 접근할 수 있는 컬럼들에 대한 권한 설정을 합니다. 인종 정보가 포함된 borrower_race컬럼들과 co_borrower_race 컬럼들을 제외한 컬럼들을 선택합니다.\n  AWS Lake Formation 서비스 콘솔로 이동합니다.   Permissions 항목 하단에 있는 Data permissions 페이지로 이동하여 Grant 버튼을 클릭합니다.   Grant permissions 페이지에서 다음의 정보를 참조하여 입력하고 Grant 버튼을 클릭합니다..\n  IAM users and roles: prv-dl-redshift-role\rDatabase: prv-dl-db\rTable: dictionaried_data\rColumns: include Columns\rinclude columns: borrower_race1~5, co_borrower_race1~5를 제외한 모든 컬\rTable permissions: Select만 선택\rRedshift 클러스터 생성   RedShift 서비스 콘솔로 이동합니다.   클러스터 생성 전에 Private subnet에 Redshift를 배포하기 위한 서브넷 그룹을 생성합니다. 구성 항목 하단에 있는 서브넷 그룹을 선택하여 클러스터 서브넷 그룹 페이지로 이동한 뒤 클러스터 서브넷 그룹 생성 버튼을 클릭합니다.   클러스터 서브넷 그룹 생성 페이지가 나오면 다음의 정보를 참조하여 서브넷 그룹을 생성합니다.\n  이름: prv-dl-private-subnet-group\r설명: RedShift private subnet group for prv dl workshop\rVPC: Target VPC\r가용영역 a에 있는 서브넷 중 Private subnet을 선택\r가용영역 c에 있는 서브넷 중 Private subnet을 선택\r이제 Redshift 클러스터를 만듭니다. 클러스터 생성 버튼을 누르고 다음의 정보를 참조하여 Redshift 클러스터를 생성합니다.  클러스터 식별자: prv-dl-dw-cluster\r마스터 사용자 이름: awsuser\r마스터 사용자 암호: awsiw2020\r데이터 베이스 이름: prv-dl-mart\r데이터 베이스 포트: 5439\r추가 구성: 기본 값 사용을 해제해서 수정이 가능하게 변경\rVPC: Target VPC\rVPC 보안 그룹: prv-dl-redshift-sg\r클러스터 서브넷 그룹: prv-dl-private-subnet-group\r향상된 VPC 라우팅: 활성화\rRedShift 접속 환경 설정 실습 환경에서 VPC의 Private 서브넷에 있는 Redshift 클러스터에 접속하기 위해서는 public subnet에 있는 bastion host VM을 통해서 접속해야합니다. 하지만 이 환경을 on-premise 데이터 센터로 구성하실 경우에는 Private IP 주소를 이용하여 바로 접속하시면 되기 때문에 bastion host 세팅이 필요하지 않습니다.\n  AWS EC2 콘솔로 이동하여 prv-dl-target-vpc-bastion 이라는 이름의 인스턴스를 살펴봅니다.   아래쪽에 있는 속성 창에 있는 보안 탭을 클릭한 뒤 보안 그룹에 있는 해당 보안 그룹을 클릭합니다.   Redshift 접속이 가능하려면 Redshift의 보안 그룹에서 bastion host에서 오는 트래픽을 허용하도록 설정이 되어있어야하고 bastion host의 보안그룹에서는 bastion host와 redshift 위치한 VPC의 CIDR block에서 들어오는 트래픽이 허용되도록 설정이 되어 있는 것이 좋습니다. 정상적으로 설정이 되어있는지 확인하고 창을 닫습니다.   SQL Workbench/J를 다운받고 설치합니다. 다운로드 링크: SQL-Workbench/J\n  AWS 공식 홈페이지에서 Redshift jdbc 드라이버를 다운받습니다.\n안내 페이지 링크: Amazon Redshift jdbc guide\n다운로드 링크: Amazon Redshift jdbc driver download\n  다운받은 Jdbc 드라이버를 SQL workbench tool에 등록합니다.   이제 Target VPC의 Bastion host를 이용해서 SSH 터널링을 합니다.\n  ssh -i .ssh/\u0026lt;your private ssh key\u0026gt; -L 5439:\u0026lt;Redshift cluster endpoint - schema 제외\u0026gt;:5439 ec2-user@\u0026lt;bastion host public ip\u0026gt;\r예: ssh -i .ssh/prv-dl.pem -L 5439:prv-dl-dw-cluster.cfrje0yghpil.ap-northeast-2.redshift.amazonaws.com:5439 ec2-user@3.34.126.248\r만들어진 터널을 통해서 SQL Workbench/J를 Redshift cluster에 연결합니다. Tool의 왼쪽 위에 있는 Create new connection profile 버튼을 누르고 새 연결 프로파일을 생성합니다. 생성 시 다음의 정보를 참조합니다.  Profile Name: prv-dl-redshift\rDirver: 앞에서 등록한 AWS Redshift JDBC 드라이버(com.amazon.redshift.jdbc.Driver)\rurl: jdbc:redshift://127.0.0.1:5439/prv-dl-mart\rUsername: awsuser\rPassword: awsiw2020\r화면의 ssh 버튼을 눌러서 나오는 ssh 연결 설정은 다음의 정보를 참조합니다.\nSSH hostname: \u0026lt;Bastion host ip\u0026gt;\rusername: ec2-user\rPrivate key file: \u0026lt;your private ssh key 경로\u0026gt;\rLocal port: 5439\rDB hostname: \u0026lt;Redshift cluster endpoint - schema 제외\u0026gt;\rDB port: 5439\rTest 버튼을 눌러서 연결을 테스트 해봅니다. 연결이 정상적일 때에도 처음에 Test를 해보면 연결이 안될 때가 있으니 두번정도 해보시는 것을 추천드립니다.\nOk 버튼을 눌러서 Workbench에 접속합니다.   RedShift Spectrum을 이용한 데이터 loading 연결이 완료되면 Redshift spectrum을 이용해서 S3에 저장된 데이터를 대상으로 쿼리합니다. 마트 테이블 생성은 다음 단계에서 진행하고 여기서는 쿼리가 정상적으로 되는지만 확인합니다.\n 현재 RedShift Schema에 있는 테이블 목록을 확인합니다. 쿼리 결과에 아무것도 나오지 않는 것을 확인합니다.  select distinct(tablename) from pg_table_def where schemaname = 'public';\rRedShift Spectrum을 통해서 S3에 저장되어 있는 데이터를 쿼리할 수 있게 Glue Data Catalog에 있는 테이블을 등록합니다.  create external schema prvdl\rfrom data catalog database 'prv-dl-db'\riam_role '\u0026lt;이전 단계에서 생성했던 prv-dl-redshift-role의 역할 ARN값\u0026gt;'\rRedshfit Spectrum으로 쿼리해서 Lake Formation에서 권한을 허용해준 컬럼들에 대한 결과들이 정상적으로 출력되는지 확인합니다.  select * from prvdl.dictionaried_data limit 20;\r실습4에서 계속  "
},
{
	"uri": "/prvdlworkshop/lab3/",
	"title": "실습4. 데이터 분석 마트 생성 및 시각화",
	"tags": [],
	"description": "",
	"content": "RedShift Spectrum을 이용해서 쿼리한 결과를 CTAS 구문을 이용하여 마트 테이블 형태로 Redshift에 저장합니다. 분석 마트 테이블은 생애 첫 주택 구입 대출자의 마트 분포, 대출 기관별 통계, 대출 목적, 투자 목적 주택 구입의 지역별 통계의 총 네개 입니다. 생성한 데이터 마트를 Amazon QuickSight를 이용하여 시각화합니다. Table of Contents  RedShift Spectrum을 이용하여 분석 마트 테이블 생성 QuickSight 환경 설정 QuickSight를 이용한 시각화 분석 QuickSight 대시 보드 생성  RedShift Spectrum을 이용하여 분석 마트 테이블 생성 SQL Workbench를 통하여 분석 마트 테이블 생성 쿼리를 수행하여 마트 테이블을 생성합니다.\n first_buyer_age_mart라는 이름으로 생애 첫 주택 구입 대출자의 마트 분포에 대한 분석 마트 테이블을 생성하고 결과를 확인합니다.  create table first_buyer_age_mart as\rSELECT age_borrower, count(first_buyer) as first_buyer_age_cnt\rfrom prvdl.dictionaried_data\rwhere first_buyer = 'Yes'\rgroup by age_borrower\rorder by age_borrower;\rselect * from first_buyer_age_mart\rorder by age_borrower;\rloan_gurantee_mart라는 이름으로 대출 기관별 통계에 대한 분석 마트 테이블을 생성하고 결과를 확인합니다.  create table loan_gurantee_mart as\rselect federal_guarntee,lien_status,avg(borrower_income) as borrower_income_avg\rfrom prvdl.dictionaried_data\rgroup by federal_guarntee,lien_status;\rselect * from loan_gurantee_mart;\rloan_mart라는 이름으로 대출 목적에 대한 분석 마트 테이블을 생성하고 결과를 확인합니다.  create table loan_mart as\rselect loan_purpose, count(*) as loan_purpose_cnt\rfrom prvdl.dictionaried_data\rgroup by loan_purpose;\rselect * from loan_mart;\rspeculation_area_mart라는 이름으로 투자 목적 주택 구입의 지역별 통계에 대한 분석 마트 테이블을 생성하고 결과를 확인합니다.  create table speculation_area_mart as\rselect country_code,count(*) as speculation_area_cnt ,avg(area_med_income) as area_med_income\rfrom prvdl.dictionaried_data\rwhere occupancy_code = 'Investment property'\rgroup by country_code\rorder by speculation_area_cnt desc;\rselect * from speculation_area_mart;\rQuickSight 환경 설정 Redshift 클러스터에 저장되어 있는 데이터 마트 테이블을 private 통신 환경으로 로딩하기 위한 네트워크 설정을 하고 데이터 시각화를 용이하게 하기 위해서 데이터를 spice 영역으로 가져오는 설정을 합니다.\n  Amazon QuickSight 서비스 콘솔로 이동합니다.   오른쪽 상단에 있는 프로필 아이콘을 누르고 QuickSight관리 버튼을 클릭합니다.   메뉴에서 VPC 연결 관리 버튼을 눌러서 VPC 연결 관리 페이지로 이동한 뒤 VPC 연결 추가 버튼을 클릭합니다.   VPC 연결 추가 페이지에서 다음의 정보를 참조하여 VPC 연결을 생성합니다.\n  VPC 연결 이름: prv-dl-quicksight-connection\rVPC ID: \u0026lt;Target VPC\u0026gt;\r보안 그룹 ID: \u0026lt;EC2 콘솔에서 CloudFormation으로 미리 생성된 prv-dl-quicksight-sg를 찾아서 ID를 복사\u0026gt;\rQuickSight 메인화면으로 나온 뒤 왼쪽에 있는 메뉴에서 데이터 세트 버튼을 클릭   "
},
{
	"uri": "/prvdlworkshop/lab4/",
	"title": "실습5. 제목",
	"tags": [],
	"description": "",
	"content": "실습에 대한 설명을 적습니다. 큰제목  순서를 가이드합니다.   "
},
{
	"uri": "/prvdlworkshop/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/prvdlworkshop/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/prvdlworkshop/credits/",
	"title": "크레딧",
	"tags": [],
	"description": "",
	"content": "패키지와 라이브러리  mermaid - generation of diagram and flowchart from text in a similar manner as markdown font awesome - the iconic font and CSS framework jQuery - The Write Less, Do More, JavaScript Library lunr - Lunr enables you to provide a great search experience without the need for external, server-side, search services\u0026hellip; horsey - Progressive and customizable autocomplete component clipboard.js - copy text to clipboard highlight.js - Javascript syntax highlighter modernizr - A JavaScript toolkit that allows web developers to use new CSS3 and HTML5 features while maintaining a fine level of control over browsers that don\u0026rsquo;t support  도구  Netlify - Continuous deployement and hosting of this documentation Hugo  "
}]