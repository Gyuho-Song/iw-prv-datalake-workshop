[
{
	"uri": "/prvdlworkshop/",
	"title": "홈",
	"tags": [],
	"description": "",
	"content": "부장님도 좋아하는 AWS의 안전한 Private Data Lake  AWS Cloud Week 2020 - Private data lake HOL workshop 안녕하세요 [부장님도 좋아하는 AWS의 안전한 Private Data Lake] 세션을 발표한 신중훈 \u0026amp; 송규호입니다. 세션 데모를 Self Phase Lab으로 제공하는 페이지입니다. 데이터 레이크를 Private link를 이용해서 구성하고 Lake Formation을 이용한 권한 관리를 실습해보실 수 있습니다. 실습을 진행할 때 사용하는 서비스에 대한 비용이 청구될 수 있는 점 유의 부탁드립니다. AWS SA와 함께 무료로 Workshop을 진행해보고 싶으신 고객분들께서는 담당 AM들에게 문의 해주시기 바랍니다.\nArchitecture  "
},
{
	"uri": "/prvdlworkshop/lab0/",
	"title": "실습1. HOL 환경 구성",
	"tags": [],
	"description": "",
	"content": "실습을 진행할 두개의 VPC(Source와 Target)를 구성합니다. Source VPC는 고객사의 On-premise를 시뮬레이션한 VPC이고 Target VPC는 Private DataLake로 사용할 VPC입니다. 각각의 VPC에는 Public subnet과 Private subnet이 있으며 외부 통신을 위한 NAT Gateway가 구성됩니다. 이 단계는 AWS CloudFormation을 이용하여 진행합니다. 이후에는 실습 데이터를 저장할 Aurora Database를 생성하고 서비스간 통신을 위한 Private endpoint를 구성합니다. Table of Contents  실습 데이터 셋 정보 VPC 생성 Aurora 데이터 베이스 생성 Aurora 셋업 및 데이터셋 로드 Private Endpoint 설정  실습 데이터셋 정보 Kaggle - Fannie Mae \u0026amp; Freddie Mac Database 2008-2018 Original dataset link\n본 실습에서는 이 중 Single Family Properties Census Tract file 2013 ~ 2018 데이터셋만을 사용했습니다. VPC 생성 실습을 진행할 VPC를 생성하고 필요한 환경설정을 진행합니다. 미리 준비된 CloudFormation을 이용하여 배포합니다.\n  아래의 링크를 클릭하여 CloudFormation Template을 수행합니다.\n  AWS Console에서 CloudFormation 수행 상태를 확인하고 Deploy가 완료되면 다음 단계로 진행합니다.\n  AWS Console에서 VPC 구성을 확인합니다.\n  Aurora 데이터 베이스 생성 원천 데이터를 저장해놓을 Aurora 데이터베이스를 생성합니다.\n  Amazon RDS 페이지로 이동합니다.   다음 정보를 참조해서 데이터 베이스를 생성합니다.\n  데이터베이스 생성 방식 선택 : 표준 생성\r엔진 옵션 : Amazon Aurora\r에디션 : MySQL과 호환되는 Amazon Aurora\r용량 유형: 프로비저닝됨\r버전 : Aurora (MySQL 5.7) 2.07.0 이상\rDB 클러스터 식별자 : prv-dl-db-cluster\r마스터 사용자 이름 : admin\r마스터 암호 : awsiw2020\r연결 VPC 정보 : 이전 단계에서 생성한 Source VPC\r데이터 베이스 인증 옵션 : 암호 인증\r추가 구성 \u0026gt; 초기 데이터베이스 이름: srcdb\r연결 VPC 정보에 특히 유의합니다. (앞 단계에서 CloudFormation으로 생성한 Source VPC를 선택해야함)  추가 연결 구성 항목을 펼쳐서 VPC 보안 그룹에 CloudFormation에서 미리 생성한 \u0026lsquo;iw-aurora-\u0026lsquo;로 시작하는 항목을 선택합니다.  Amazon Aurora 클러스터가 생성되면 엔드포인트를 따로 복사합니다.  Aurora 셋업 및 데이터셋 로드 실습에 사용할 데이터셋을 다운받아서 Aurora 데이터베이스로 적재합니다.\n Aurora 데이터베이스에 접속하여 작업을 할 EC2 Instance에 ssh로 접속합니다. EC2 정보는 EC2 console에서 확인할 수 있습니다.   ssh -i \u0026lt;ssh private key\u0026gt; ec2-user@\u0026lt;instance ip or dns name\u0026gt;\rAWS cli 설정을 합니다.  aws configure\r실습에서 사용할 데이터를 다운로드 하고 압축을 풉니다.  wget \u0026quot;https://private-data-lake-iw2020.s3.ap-northeast-2.amazonaws.com/sfct_dataset/SFCT.tar.gz\u0026quot;\rtar xvfz SFCT.tar.gz\rEC2에 mysql을 설치하고 Aurora에 접속합니다. 아까 복사해둔 Aurora database의 endpoint를 이용합니다.  sudo yum install mysql\rmysql -h \u0026lt;Aurora database endpoint\u0026gt; -P 3306 -u admin -p\r암호를 묻는 창에는 아까 설정한 \u0026lsquo;awsiw2020\u0026rsquo;를 입력합니다.\nDatabase schema와 user를 생성합니다.  CREATE DATABASE estate;\rCREATE USER 'user1'@'%' IDENTIFIED BY 'password1';\rGRANT SELECT, INSERT, UPDATE, DELETE, CREATE, DROP, RELOAD, PROCESS, REFERENCES, INDEX, ALTER, SHOW DATABASES, CREATE TEMPORARY TABLES, LOCK TABLES, EXECUTE, REPLICATION SLAVE, REPLICATION CLIENT, CREATE VIEW, SHOW VIEW, CREATE ROUTINE, ALTER ROUTINE, CREATE USER, EVENT, TRIGGER ON *.* TO 'user1'@'%' WITH GRANT OPTION;\rTable을 생성하고 데이터를 Table로 load합니다.  USE estate;\rCREATE TABLE SFCT (\rent_flag int,\rrecord_num int,\rus_pos_code int,\rmsa_code int,\rcountry_code int,\rcensus_tract int,\rminority_pct float,\rmedian_income int,\rlocal_med_income int,\rincome_ratio float,\rborrower_income int,\rarea_med_income int,\rborrower_income_ratio float,\rupb int,\rloan_purpose int,\rfederal_guarntee int,\rnum_borrower int,\rfirst_buyer int,\rborrower_race1 int,\rborrower_race2 int,\rborrower_race3 int,\rborrower_race4 int,\rborrower_race5 int,\rborrower_ethnicity int,\rco_borrower_race1 int,\rco_borrower_race2 int,\rco_borrower_race3 int,\rco_borrower_race4 int,\rco_borrower_race5 int,\rco_borrower_ethnicity int,\rborrower_gender int,\rco_borrower_gender int,\rage_borrower int,\rage_co_borrower int,\roccupancy_code int,\rrate_spread float,\rhoepa_status int,\rproperty_type int,\rlien_status int\r);\rLOAD DATA LOCAL INFILE '/home/ec2-user/SFCT/2013/fhlmc_sf2013c_loans.txt' INTO TABLE SFCT\rFIELDS TERMINATED BY ' '\rOPTIONALLY ENCLOSED BY '\u0026quot;';\rLOAD DATA LOCAL INFILE '/home/ec2-user/SFCT/2013/fnma_sf2013c_loans.txt' INTO TABLE SFCT\rFIELDS TERMINATED BY ' '\rOPTIONALLY ENCLOSED BY '\u0026quot;';\rLOAD DATA LOCAL INFILE '/home/ec2-user/SFCT/2014/fhlmc_sf2014c_loans.txt' INTO TABLE SFCT\rFIELDS TERMINATED BY ' '\rOPTIONALLY ENCLOSED BY '\u0026quot;';\rLOAD DATA LOCAL INFILE '/home/ec2-user/SFCT/2014/fnma_sf2014c_loans.txt' INTO TABLE SFCT\rFIELDS TERMINATED BY ' '\rOPTIONALLY ENCLOSED BY '\u0026quot;';\rLOAD DATA LOCAL INFILE '/home/ec2-user/SFCT/2015/fhlmc_sf2015c_loans.txt' INTO TABLE SFCT\rFIELDS TERMINATED BY ' '\rOPTIONALLY ENCLOSED BY '\u0026quot;';\rLOAD DATA LOCAL INFILE '/home/ec2-user/SFCT/2015/fnma_sf2015c_loans.txt' INTO TABLE SFCT\rFIELDS TERMINATED BY ' '\rOPTIONALLY ENCLOSED BY '\u0026quot;';\rLOAD DATA LOCAL INFILE '/home/ec2-user/SFCT/2016/fhlmc_sf2016c_loans.txt' INTO TABLE SFCT\rFIELDS TERMINATED BY ' '\rOPTIONALLY ENCLOSED BY '\u0026quot;';\rLOAD DATA LOCAL INFILE '/home/ec2-user/SFCT/2016/fnma_sf2016c_loans.txt' INTO TABLE SFCT\rFIELDS TERMINATED BY ' '\rOPTIONALLY ENCLOSED BY '\u0026quot;';\rLOAD DATA LOCAL INFILE '/home/ec2-user/SFCT/2017/fhlmc_sf2017c_loans.txt' INTO TABLE SFCT\rFIELDS TERMINATED BY ' '\rOPTIONALLY ENCLOSED BY '\u0026quot;';\rLOAD DATA LOCAL INFILE '/home/ec2-user/SFCT/2017/fnma_sf2017c_loans.txt' INTO TABLE SFCT\rFIELDS TERMINATED BY ' '\rOPTIONALLY ENCLOSED BY '\u0026quot;';\rLOAD DATA LOCAL INFILE '/home/ec2-user/SFCT/2018/fhlmc_sf2018c_loans.txt' INTO TABLE SFCT\rFIELDS TERMINATED BY ' '\rOPTIONALLY ENCLOSED BY '\u0026quot;';\rLOAD DATA LOCAL INFILE '/home/ec2-user/SFCT/2018/fnma_sf2018c_loans.txt' INTO TABLE SFCT\rFIELDS TERMINATED BY ' '\rOPTIONALLY ENCLOSED BY '\u0026quot;';\rTable에 저장된 데이터를 살펴봅니다. Dictionary 데이터를 매핑하기 전이기 때문에 모든 값이 숫자형으로 되어 있는 것을 확인할 수 있습니다.  select * from SFCT limit 10;\rPrivate Endpoint 설정 AWS의 내부망을 사용하여 데이터를 전송하는데 꼭 필요한 Private Endpoint들을 생성합니다. S3와 Glue의 엔드포인트가 필요합니다.\n  AWS Console에서 VPC Service로 이동합니다.   엔드포인트 항목을 선택하여 S3 endpoint를 만듭니다. 주의! VPC 선택창에서 실습에서 사용할 Target VPC를 꼭 선택하셔야 합니다.   Glue endpoint도 만듭니다. 마찬가지로 VPC 선택창에서 실습에서 사용할 Target VPC를 꼭 선택하셔야 합니다.    "
},
{
	"uri": "/prvdlworkshop/lab1/",
	"title": "실습2. 데이터 탐색 및 ETL",
	"tags": [],
	"description": "",
	"content": "Glue를 사용하기 위한 환경 설정을 하고 이를 이용해서 데이터를 Loading합니다. 이후에는 Glue Dev Endpoint 노트북을 이용하여 Dictionary 데이터를 매핑시키는 데이터 ETL 작업을 진행합니다. Table of Contents  Glue 실행을 위한 IAM role 생성 Glue Security Group 생성 Glue를 이용한 데이터 Crawling Glue를 이용한 데이터 loading 작업 Glue Dev endpoint Notebook을 활용한 Dictionary 데이터 매핑  Glue 실행을 위한 IAM role 생성 Glue를 실행하기 위한 IAM role을 생성합니다.\n  AWS 서비스 콘솔에서 IAM service로 이동합니다.   역할만들기 유형에서 AWS 서비스가 선택되어 있는지 확인합니다.   화면 하단에서 Glue를 선택합니다.   권한 정책 연결에서 다음의 정책들을 선택합니다.\n  AmazonRDSFullAccess\rAmazonS3FullAccess\rAWSGlueServiceRole\r역할 이름에 prv-dl-workshop-role을 입력하고 생성합니다.   Glue Security Group 생성 Glue의 데이터 통신을 위한 Security Group 설정을 합니다.\n AWS 콘솔에서 VPC서비스로 이동한 뒤 Security Group에서 Glue가 사용할 Security Group을 생성합니다.  보안 그룹 생성 버튼을 눌러서 Security Group을 만듭니다. 하단의 정보를 참고합니다.  보안 그룹 이름: prv-dl-glue-sg\r설명: Security Group for Glue ETL in prv dl workshop\rVPC: Target VPC\r보안 그룹이 생성되면 인바운드 규칙 편집 버튼을 눌러서 self referencing 룰을 추가해줍니다. 방금 생성한 Glue Security Group자신을 소스로하는 모든 트래픽을 허용하는 룰입니다.   Glue를 이용한 데이터 Crawling Glue를 이용해서 데이터를 처리하려면 데이터 소스에 대한 정보를 등록해주는 작업을 선행해야합니다.\n AWS Glue 서비스 콘솔로 이동한 뒤 연결 페이지로 이동합니다. 연결 추가를 눌러서 Aurora 데이터베이스의 Connection 정보를 등록합니다.  연결 구성에 다음의 정보를 참조합니다.  연결 이름: prv-dl-aurora\r연결 유형: Amazon RDS\r데이터 베이스 엔진: Amazon Aurora\r인스턴스: prv-dl-db-cluster-instance-1\r데이터베이스 이름: srcdb\r사용자 이름: admin\r암호: awsiw2020\r다음으로는 크롤러를 추가해줍니다. 크롤러 페이지에서 크롤러 추가 버튼을 클릭합니다.  다음의 정보를 참조하여 크롤러를 생성합니다.  크롤러 이름: prv-dl-aurora-crawler\rCrawler source type: Data stores\rRepeat crawls of S3 data stores: Crawl all folders\r데이터 스토어 선택: JDBC\r연결: prv-dl-aurora\r포함경로: estate/%\r다른 데이터 스토어 추가: 아니요\rIAM 역할: prv-dl-workshop-role\r빈도: 온디맨드 실\r데이터 베이스: 데이터베이스 추가 버튼을 누르고 prv-dl-db 입력\r방금 생성한 크롤러를 선택하고 크롤러 실행 버튼을 클릭    "
},
{
	"uri": "/prvdlworkshop/lab2/",
	"title": "실습3. 제목",
	"tags": [],
	"description": "",
	"content": "실습에 대한 설명을 적습니다. 큰제목  순서를 가이드합니다.   "
},
{
	"uri": "/prvdlworkshop/lab3/",
	"title": "실습4. 제목",
	"tags": [],
	"description": "",
	"content": "실습에 대한 설명을 적습니다. 큰제목  순서를 가이드합니다.  "
},
{
	"uri": "/prvdlworkshop/lab4/",
	"title": "실습5. 제목",
	"tags": [],
	"description": "",
	"content": "실습에 대한 설명을 적습니다. 큰제목  순서를 가이드합니다.   "
},
{
	"uri": "/prvdlworkshop/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/prvdlworkshop/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/prvdlworkshop/credits/",
	"title": "크레딧",
	"tags": [],
	"description": "",
	"content": "패키지와 라이브러리  mermaid - generation of diagram and flowchart from text in a similar manner as markdown font awesome - the iconic font and CSS framework jQuery - The Write Less, Do More, JavaScript Library lunr - Lunr enables you to provide a great search experience without the need for external, server-side, search services\u0026hellip; horsey - Progressive and customizable autocomplete component clipboard.js - copy text to clipboard highlight.js - Javascript syntax highlighter modernizr - A JavaScript toolkit that allows web developers to use new CSS3 and HTML5 features while maintaining a fine level of control over browsers that don\u0026rsquo;t support  도구  Netlify - Continuous deployement and hosting of this documentation Hugo  "
}]